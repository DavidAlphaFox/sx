\chapter{Garbage Collection algorithm}

\section{The local reference counting algorithm}

The idea is that the i\textsuperscript{th} replica of a volume communicates information
about hashes to the i\textsuperscript{th} hash replica nodes.

That way we don't need any synchronization between the replicas when GC-ing
hashes.

When information about a file gets replicated to the n\textsuperscript{th}
volume replica, it is as if a new file got uploaded, similarly when a file gets
deleted.

\subsection{GC information per hash}
Each hash will also contain 1 counter, and a last reserved timestamp.
The last reserved timestamp is just an optimization to be able to GC files that
never finished uploading without having to wait for the tokens to get GC-ed
first.

\subsection{Initial PUT}

We check hash presence as before, but also send a batched message to all nodes
that contain the i\textsuperscript{th} replica of the hash, i.e.
\begin{verbatim}
 send(hdist(hash, i), RESERVE)
\end{verbatim}

<<comment:declarations>>=
#define M 6
mtype = { RESERVE }
typedef hash { int id; }
chan hash-nodes = [0] of { mtype, hash }
@

<<comment:initial-put>>=
active proctype put()
{
    int hashid;
    hash h;
    /* random hash */
    hashid = 0;
    do
        :: skip -> break
        :: hashid < M-1 -> hashid++
    od;
    h.id = hashid;

    hash-nodes ! RESERVE, h
}

@ Optimization: we can send this message only to hashes that are already present.
We can set the RESERVED flag when the hash is actually uploaded.
When the RESERVE command is received by a hash node it refreshes the last-reserved
timestamp.

Implementation possibilities:
\begin{itemize}
\item introduce new API for reservations
    \begin{itemize}
    \item after calling HEAD for the hash batch, we'd call the reserve API
    \item this means an additional API call, and additional latency in the
    upload path
    \end{itemize}
\item use a flag to make the HEAD update the last-used timestamp on hashes
    \begin{itemize}
    \item disadvantage: not nice, as HEAD is an idempotent operation, and is supposed to be
    read-only
    \item we can think of our last-reserved timestamp as last-used timestamp,
    i.e. like the \emph{atime} of a file, which does get updated even on
    readonly operations
    \item to avoid DoS we would allow updating the timestamp only for s2s, not
    by the clients
    \item and there is our flag: cluster-originated (uid=0) HEAD operations update the last-used
    timestamp on hashes, anything else behaves as before
    \item advantage: no additional operation needed, failures are already
    handled: if we can't reserve a hash, it is as if the hash is not there and
    the nodes already know to ask for the hash to be uploaded again
    \item sqlite implementation: start a transaction, do the queries and
    reservations in that transaction and finish the batch with a single commit
    \item \emph{time needed}: 1.5 days impl., perf. testing 2 days
    \end{itemize}
\item use a PUT (or POST?) request instead of HEAD
    \begin{itemize}
    \item PUT /.data/<blocksize>/<hashbatch>?o=reserve, empty body, reply like for HEAD
    \item can be reused for other operations (see inuse, delete below)
    \item needs updating the head\_check API to take additional parameters: verb
    and query, everything else can be reused
    \item disadvantage: not very RESTful to use o=, but then we already use it
    in various other places, at least we'd be consistent if we use it here as
    well (the pure REST way would be another verb, but we're limited to what
          http offers)
    \item \emph{additional time needed}: 0.5 days impl.
    \end{itemize}
\end{itemize}

Preferred: the latter (i.e. PUT/POST instead of HEAD)

\subsection{Final PUT}

After we have successfully committed the file to the volume, we send a message
to all nodes that contain the i\textsuperscript{th} hash:
\begin{verbatim}
 send(hdist(hash, i), INUSE)
\end{verbatim}

Processing the INUSE command will increment the inuse counter.
Optimization: reset the last-reserved timestamp, otherwise even if the file
was deleted we'd have to wait for it to expire before being able to GC it.

Implementation details:
\begin{itemize}
\item currently there is a job that checks the hashes after the final PUT.
\item instead of simply checking for the hash's presence, we need to update
their inuse flag
\item we could have a separate API, or abuse HEAD again
\item or we could reuse the o= API above (preferred)
    \begin{itemize}
    \item PUT /.data/<blocksize>/<hashbatch>?o=INUSE
    \item if any hash is missing it'll reply with an error, that can then be
    passed on to the user
    \item if we have the head\_check API modification from the initial PUT then
    implementing this should be easy (we should call it somelike like:
                                      hash\_batch\_op instead of head\_check
                                      though)
    \item \emph{time needed}: INUSE API impl: 1.5 days, "client" i.e. jobmgr
    part: 1 days, perf testing: can be coupled with initial PUT
    \end{itemize}
\end{itemize}

\subsection{DELETE}

When a file gets deleted we send a message to all nodes that contain the
i\textsuperscript{th} hash:
\begin{verbatim}
 send(hdist(hash, i), DELETE)
\end{verbatim}

It decrements the inuse counter.

Implementation:
\begin{itemize}
\item similar to previous point, with ?o=DELETE
\item deleting a file doesn't have polling now, so we'll need to implement that
as well
\item \emph{time needed}: DELETE polling/jobmgr: 3 days, actual API: 1 days
\end{itemize}

\subsection{Hash and file replication}

No special action: it is just like uploading a brand new file.
Hashes are always synced before the file itself (file can start replicating
after the upload is finished only).

Note: hash might get uploaded to another replica first and then replicated back
to the i-th one.

Implementation: IF file replication is implemented as described in this doc,
    then nothing else needs to be done, i.e.:
\begin{itemize}
\item initial PUT to volume node 1
\item requests hashes from client, client uploads hashes to data nodes X
\item data replication starts
\item final PUT waits till replication completes on volume node 1
\item initiates replication of the file (and filemeta) to the other volume
nodes, and immediately does a final PUT there as well (can be done partially
                                                       in parallel with previous
                                                       step)
\end{itemize}

Some deviations are allowed from this design without affecting the GC, the main
idea is that the i\textsuperscript{th} volume node replicas, and the
$i$\textsuperscript{th} data node replicas act indepedently from the
$i+1$\textsuperscript{th}, and that there is an initial PUT, and final PUT on
each volume node replica.

If not then this needs to be partially implemented or another solution found:
\emph{time needed} 4-5 days.

\subsection{Rebalance}

When rebalance moves a hash it needs to move all its metadata (counters and
 reservation timestamp) together with it.
After a hash is successfully moved to another node it can be immediately deleted
from the local node.

Implementation: rebalance is out of scope for now.

\subsection{GC}

Use the index on inuse counter to find GC candidates, then search for those
without a reserved timestamp, or with an expired reservation: mark them for GC.
They will get deleted in the next GC cycle.
In the current GC cycle delete hashes that got marked in the previous cycle.

Marker: to avoid introducing yet more columns we can use a special value for the
inuse counter like -100 to mark for GC.

Implementation:
\begin{itemize}
\item a separate jobmgr (like blockmgr), it could be shared with the rebalancer
or independent
\item would reuse the jobmgr APIs
\item would run in a cron-like fashion, but should have some self-throttling
mechanism: i.e. if node is sustaining a high number of failures / load, delay
the GC cycle (perhaps a feature after 1.0 only?)
\item the GC cycle is \emph{local} to each node, it requires no coordinator!
\item it is based purely on the last-used (last-reserved) timestamp, and the
inuse counters
\item \emph{time needed}: job stuff: 3 days, sqlite stuff: 1 day
\end{itemize}

\subsection{Reusing freed disk space}

Implementation:
\begin{itemize}
\item We also need to actually free up the disk-space
\item during hash uploads we need to maintain and select blocks from a
free-list, instead of simply picking the max offsets
\item this requires tinkering with the sqlite query used during hash upload, and
its transaction
\item has to be done carefuly to preserve scalability, i.e. don't hold sqlite
lock for too long
\item ideally we could do some operations on the DB, release the lock, actually write the
batched hash(es) to the disk, then do some more operations on the DB, and then
reply; allowing the other fcgis to write to the DB while we're busy writing the
data to disk
\item also what about f(data)sync? at some point (when replication is finished)
    we should f(data)sync, or at least periodically at a configurable interval,
    otherwise we could think our data is nicely replicated on 5 nodes, and after
    a power outage the data from all 5 nodes would be lost: yet our DB would
    claim we have it, and we wouldn't be able to self-heal and ask future
    clients to upload the hashes again. This could be admin-configurable
    cluster-wide (i.e. no fsync for data, periodic fsync, fsync after each file
                  PUT)
\item \emph{time needed}: sqlite tinkering 2 days, performance testing
(incl fsync) 3 days
\end{itemize}

\subsection{Token expiration}

Each token has a progress/expiration timestamp.

Implementation: (gc job cleans it + code to maintain expiration/progress) \emph{time needed} 2 days

\subsection{Automated tests}

We currently lack replication tests.
Implementation:
\begin{itemize}
\item something on Mininet
\item setup nodes with replicas, test just the basics: \emph{time needed} 5 days
\item race tests (i.e. upload while already deleted/deleting), \emph{time
    needed}: 7 days
\item performance tests on cowboys: 3-4 days
\end{itemize}

Of course we also need to test the previous APIs when
they're all together, which would take another 3-4 days.

\subsection{Promela model}

This document describes the algorithm but also defines a Promela model
to validate some properties.
TODO: out of scope for now

\subsection{Pro and cons}
Pros:
\begin{itemize}
\item decentralized, each node collects its own hashes
\end{itemize}

Cons:
\begin{itemize}
\item if volume and hash nodes loose connection then you cannot upload files
(if we allow uploads we risk GC-ing uploaded files if the volume can't
 communicate with hash node). You wouldn't be able to finish uploading anyway,
    so this is not much of a practical limitation
\item need to keep a reverse map all the time
\item reference counting means we have bit more overhead when creating files,
but we compensate that by avoiding a LOT of overhead when GC-ing
\item more inter-node traffic during upload
\end{itemize}
